\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plos2015}
\citation{Huttenhower2012}
\citation{Gilbert2014}
\citation{Zhang2015}
\citation{Alneberg2014,Eren2015}
\citation{Anantharaman2016}
\citation{Kanehisa2017}
\citation{Finn2016}
\citation{Tatusov1997}
\citation{Langille2013}
\citation{Louca2016,Louca2017}
\citation{Louca2016a}
\citation{Whitman}
\citation{Leslie2002,Cai2003,Someya2010}
\citation{Asgari2015}
\citation{Weimann2016}
\citation{Berger2005}
\citation{Louca2016}
\citation{Hyatt2010}
\citation{Buchfink2014}
\citation{Hastie2009a,Freedman2009}
\citation{Lee2006}
\citation{Lello2017}
\citation{Hastie2009}
\citation{Fawcett2006,Flach2011}
\newlabel{tab1}{{\caption@xref {tab1}{ on input line 322}}{6}{Gene orthologs used by classifiers}{table.caption.15}{}}
\newlabel{tab1}{{\caption@xref {tab1}{ on input line 331}}{6}{Gene orthologs used by classifiers}{table.caption.15}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces {\bf  Details of classifiers for specific functions.} Tables showing the all nonzero weights in the logistic regression models trained on three functions from the FAPROTAX database. Note that there are 9647 KEGG orthologs used in our models, so the vast majority of weights are set to zero in these models.\relax }}{6}{table.caption.15}}
\newlabel{tab1}{{1}{6}{{\bf Details of classifiers for specific functions.} Tables showing the all nonzero weights in the logistic regression models trained on three functions from the FAPROTAX database. Note that there are 9647 KEGG orthologs used in our models, so the vast majority of weights are set to zero in these models.\relax }{table.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces {\bf  Overall performance of classification algorithms.} The AUROC score on each classification task (each function) is shown for two classification algorithms: $\ell $1-regularized logistic regression and the random forest. Functions are ordered by the LR score.\relax }}{7}{figure.caption.13}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig1}{{1}{7}{{\bf Overall performance of classification algorithms.} The AUROC score on each classification task (each function) is shown for two classification algorithms: $\ell $1-regularized logistic regression and the random forest. Functions are ordered by the LR score.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces {\bf  KEGG modules for some functions.} Representations of the KEGG modules corresponding to the FAPROTAX functions shown in Table\nobreakspace  {}\ref  {tab1}. Modules are organized into `blocks' of orthologs, typically indicating a protein complex. Orthologs positioned next to each other are `options', i.e. that section of the module is present if any of the adjacent blocks are present.\relax }}{8}{figure.caption.16}}
\newlabel{fig2}{{2}{8}{{\bf KEGG modules for some functions.} Representations of the KEGG modules corresponding to the FAPROTAX functions shown in Table~\ref {tab1}. Modules are organized into `blocks' of orthologs, typically indicating a protein complex. Orthologs positioned next to each other are `options', i.e. that section of the module is present if any of the adjacent blocks are present.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces {\bf  Classifier performance and complexity.} Scatterplot showing the AUROC score of the different classifiers plotted against the number of gene orthologs the classifier uses to make its predictions. Point size is proportional to the number of positive examples in the training set.\relax }}{8}{figure.caption.17}}
\newlabel{fig3}{{3}{8}{{\bf Classifier performance and complexity.} Scatterplot showing the AUROC score of the different classifiers plotted against the number of gene orthologs the classifier uses to make its predictions. Point size is proportional to the number of positive examples in the training set.\relax }{figure.caption.17}{}}
\citation{Martiny2015}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces {\bf  Comparison of classifiers to KEGG modules.} Table showing the performance of using KEGG module presence/absence against LR classifiers for some functions where equivalent KEGG modules exist. Since the KEGG module approach doesn't give a probability, the AUROC score can't be used, so the F1 score and confusion matrices are comapred.\relax }}{9}{table.caption.19}}
\newlabel{tab2}{{2}{9}{{\bf Comparison of classifiers to KEGG modules.} Table showing the performance of using KEGG module presence/absence against LR classifiers for some functions where equivalent KEGG modules exist. Since the KEGG module approach doesn't give a probability, the AUROC score can't be used, so the F1 score and confusion matrices are comapred.\relax }{table.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces {\bf  Taxonomic distribution of metabolic traits.} Taxonomic trees of all prokaryotic NCBI species with full genomes. For training the cross-taxa verison of the classifier, only the Proteobacteria (red section of the tree) were used, and the models were tested on the rest of the tree. Species capable of a) sulfate respiration and b) nitrate respiration are highlighted on the trees.\relax }}{9}{figure.caption.21}}
\newlabel{fig4}{{4}{9}{{\bf Taxonomic distribution of metabolic traits.} Taxonomic trees of all prokaryotic NCBI species with full genomes. For training the cross-taxa verison of the classifier, only the Proteobacteria (red section of the tree) were used, and the models were tested on the rest of the tree. Species capable of a) sulfate respiration and b) nitrate respiration are highlighted on the trees.\relax }{figure.caption.21}{}}
\bibstyle{plos2015}
\bibdata{trait_prediction,others}
\bibcite{Huttenhower2012}{1}
\newlabel{S1_Fig}{{}{10}{S1 Fig}{section*.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces {\bf  Overall performance of classification algorithm in the cross-taxoa case.} The AUROC score on each classification task (each function) is shown for the LR classifiers based on KOs for the case of random train/test splitting as in Figure\nobreakspace  {}\ref  {fig1}, and a classifier trained on the Proteobacteria and tested on all other organisms in the dataset.\relax }}{11}{figure.caption.22}}
\newlabel{fig5}{{5}{11}{{\bf Overall performance of classification algorithm in the cross-taxoa case.} The AUROC score on each classification task (each function) is shown for the LR classifiers based on KOs for the case of random train/test splitting as in Figure~\ref {fig1}, and a classifier trained on the Proteobacteria and tested on all other organisms in the dataset.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces {\bf  Classifier performance and complexity, cross-taxa case.} Scatterplot showing the AUROC score of the different classifiers plotted against the number of gene orthologs used for the cross-taxa classifiers. Point size is proportional to the number of positive examples in the training set.\relax }}{12}{figure.caption.23}}
\newlabel{fig6}{{6}{12}{{\bf Classifier performance and complexity, cross-taxa case.} Scatterplot showing the AUROC score of the different classifiers plotted against the number of gene orthologs used for the cross-taxa classifiers. Point size is proportional to the number of positive examples in the training set.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces {\bf  Heatmap of presence/absence of fucntions in MAGs.} Results of running the set of LR classsifiers trained on NCBI genomes on MAGs assembled from two environments: laboratory anaerobic digesters and the ocean.\relax }}{12}{figure.caption.25}}
\newlabel{heatmap}{{7}{12}{{\bf Heatmap of presence/absence of fucntions in MAGs.} Results of running the set of LR classsifiers trained on NCBI genomes on MAGs assembled from two environments: laboratory anaerobic digesters and the ocean.\relax }{figure.caption.25}{}}
\bibcite{Gilbert2014}{2}
\bibcite{Zhang2015}{3}
\bibcite{Alneberg2014}{4}
\bibcite{Eren2015}{5}
\bibcite{Anantharaman2016}{6}
\bibcite{Kanehisa2017}{7}
\bibcite{Finn2016}{8}
\bibcite{Tatusov1997}{9}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces {\bf  Overall comparison of AD and Tara MAGs.} Proportions of MAGS from the two environments having a function, for some of the most common functions.\relax }}{13}{figure.caption.26}}
\newlabel{mag-compare}{{8}{13}{{\bf Overall comparison of AD and Tara MAGs.} Proportions of MAGS from the two environments having a function, for some of the most common functions.\relax }{figure.caption.26}{}}
\bibcite{Langille2013}{10}
\bibcite{Louca2016}{11}
\bibcite{Louca2017}{12}
\bibcite{Louca2016a}{13}
\bibcite{Whitman}{14}
\bibcite{Leslie2002}{15}
\bibcite{Cai2003}{16}
\bibcite{Someya2010}{17}
\bibcite{Asgari2015}{18}
\bibcite{Weimann2016}{19}
\bibcite{Berger2005}{20}
\bibcite{Hyatt2010}{21}
\bibcite{Buchfink2014}{22}
\bibcite{Hastie2009a}{23}
\bibcite{Freedman2009}{24}
\bibcite{Lee2006}{25}
\bibcite{Lello2017}{26}
\bibcite{Hastie2009}{27}
\bibcite{Fawcett2006}{28}
\bibcite{Flach2011}{29}
\bibcite{Martiny2015}{30}
\newlabel{LastPage}{{}{15}{}{page.15}{}}
\xdef\lastpage@lastpage{15}
\xdef\lastpage@lastpageHy{15}
