\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plos2015}
\citation{Huttenhower2012}
\citation{Gilbert2014}
\citation{Zhang2015}
\citation{Alneberg2014,Eren2015}
\citation{Anantharaman2016}
\citation{Kanehisa2017}
\citation{Finn2016}
\citation{Tatusov1997}
\citation{Louca2016,Louca2017}
\citation{Louca2016a}
\citation{Whitman}
\citation{Leslie2002,Cai2003,Someya2010}
\citation{Asgari2015}
\citation{Weimann2016}
\citation{Berger2005}
\citation{Louca2016}
\citation{Hyatt2010}
\citation{Buchfink2014}
\citation{Eddy2011}
\citation{Finn2016}
\citation{Hastie2009a,Freedman2009}
\citation{Lee2006}
\citation{Lello2017}
\citation{Hastie2009}
\citation{Hastie2009b}
\citation{Fawcett2006,Flach2011}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces {\bf  Overall performance of classification algorithms.} The AUROC score on each classification task (each function) is shown for three classification algorithms: $\ell $1-regularized logistic regression, the random forest and a linear support vector machine (SVM). Functions are ordered by the LR score.\relax }}{6}{figure.caption.13}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig1}{{1}{6}{{\bf Overall performance of classification algorithms.} The AUROC score on each classification task (each function) is shown for three classification algorithms: $\ell $1-regularized logistic regression, the random forest and a linear support vector machine (SVM). Functions are ordered by the LR score.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces {\bf  Performance of classification algorithms using different ortholog schemes.} The AUROC score on each classification task (each function) is shown for algorithms trained on two representations of genomes in terms of orthologous groups of genes, the KEGG orthology (KO) and Pfam protein families.\relax }}{7}{figure.caption.14}}
\newlabel{fig1}{{2}{7}{{\bf Performance of classification algorithms using different ortholog schemes.} The AUROC score on each classification task (each function) is shown for algorithms trained on two representations of genomes in terms of orthologous groups of genes, the KEGG orthology (KO) and Pfam protein families.\relax }{figure.caption.14}{}}
\newlabel{tab1}{{\caption@xref {tab1}{ on input line 337}}{8}{Gene orthologs used by classifiers}{table.caption.16}{}}
\newlabel{tab1}{{\caption@xref {tab1}{ on input line 346}}{8}{Gene orthologs used by classifiers}{table.caption.16}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces {\bf  Details of classifiers for specific functions.} Tables showing the all nonzero weights in the logistic regression models trained on three functions from the FAPROTAX database. Note that there are 9647 KEGG orthologs used in our models, so the vast majority of weights are set to zero in these models.\relax }}{8}{table.caption.16}}
\newlabel{tab1}{{1}{8}{{\bf Details of classifiers for specific functions.} Tables showing the all nonzero weights in the logistic regression models trained on three functions from the FAPROTAX database. Note that there are 9647 KEGG orthologs used in our models, so the vast majority of weights are set to zero in these models.\relax }{table.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces {\bf  KEGG modules for some functions.} Representations of the KEGG modules corresponding to the FAPROTAX functions shown in Table\nobreakspace  {}\ref  {tab1}. Modules are organized into `blocks' of orthologs, typically indicating a protein complex. Orthologs positioned next to each other are `options', i.e. that section of the module is present if any of the adjacent blocks are present.\relax }}{9}{figure.caption.17}}
\newlabel{fig2}{{3}{9}{{\bf KEGG modules for some functions.} Representations of the KEGG modules corresponding to the FAPROTAX functions shown in Table~\ref {tab1}. Modules are organized into `blocks' of orthologs, typically indicating a protein complex. Orthologs positioned next to each other are `options', i.e. that section of the module is present if any of the adjacent blocks are present.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces {\bf  Classifier performance and complexity.} Scatterplot showing the AUROC score of the different classifiers plotted against the number of gene orthologs the classifier uses to make its predictions. Point size is proportional to the number of positive examples in the training set.\relax }}{9}{figure.caption.18}}
\newlabel{fig3}{{4}{9}{{\bf Classifier performance and complexity.} Scatterplot showing the AUROC score of the different classifiers plotted against the number of gene orthologs the classifier uses to make its predictions. Point size is proportional to the number of positive examples in the training set.\relax }{figure.caption.18}{}}
\citation{Martiny2015}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces {\bf  Comparison of classifiers to KEGG modules.} Table showing the performance of using KEGG module presence/absence against LR classifiers for some functions where equivalent KEGG modules exist. Since the KEGG module approach does not give a probability, the AUROC score cannot be used, so the F1 score and confusion matrices are compared.\relax }}{10}{table.caption.20}}
\newlabel{tab2}{{2}{10}{{\bf Comparison of classifiers to KEGG modules.} Table showing the performance of using KEGG module presence/absence against LR classifiers for some functions where equivalent KEGG modules exist. Since the KEGG module approach does not give a probability, the AUROC score cannot be used, so the F1 score and confusion matrices are compared.\relax }{table.caption.20}{}}
\citation{Zhang2015}
\citation{Anantharaman2016}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces {\bf  Taxonomic distribution of metabolic traits.} Taxonomic trees of all prokaryotic NCBI species with full genomes. For training the cross-taxa verison of the classifier, only the Proteobacteria (red section of the tree) were used, and the models were tested on the rest of the tree. Species capable of a) sulfate respiration and b) nitrate respiration are highlighted on the trees.\relax }}{11}{figure.caption.22}}
\newlabel{fig4}{{5}{11}{{\bf Taxonomic distribution of metabolic traits.} Taxonomic trees of all prokaryotic NCBI species with full genomes. For training the cross-taxa verison of the classifier, only the Proteobacteria (red section of the tree) were used, and the models were tested on the rest of the tree. Species capable of a) sulfate respiration and b) nitrate respiration are highlighted on the trees.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces {\bf  Overall performance of classification algorithm in the cross-taxa case.} The AUROC score on each classification task (each function) is shown for the LR classifiers based on KOs for the case of random train/test splitting as in Figure\nobreakspace  {}\ref  {fig1}, and a classifier trained on the Proteobacteria and tested on all other organisms in the dataset.\relax }}{12}{figure.caption.23}}
\newlabel{fig5}{{6}{12}{{\bf Overall performance of classification algorithm in the cross-taxa case.} The AUROC score on each classification task (each function) is shown for the LR classifiers based on KOs for the case of random train/test splitting as in Figure~\ref {fig1}, and a classifier trained on the Proteobacteria and tested on all other organisms in the dataset.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces {\bf  Classifier performance and complexity, cross-taxa case.} Scatterplot showing the AUROC score of the different classifiers plotted against the number of gene orthologs used for the cross-taxa classifiers. Point size is proportional to the number of positive examples in the training set.\relax }}{13}{figure.caption.24}}
\newlabel{fig6}{{7}{13}{{\bf Classifier performance and complexity, cross-taxa case.} Scatterplot showing the AUROC score of the different classifiers plotted against the number of gene orthologs used for the cross-taxa classifiers. Point size is proportional to the number of positive examples in the training set.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces {\bf  Heatmap of presence/absence of fucntions in MAGs.} Results of running the set of LR classsifiers trained on NCBI genomes on MAGs assembled from three environments: laboratory anaerobic digesters, the ocean and `candidate phyla radiation' (CPR) organisms from an aquifer system.\relax }}{14}{figure.caption.26}}
\newlabel{heatmap}{{8}{14}{{\bf Heatmap of presence/absence of fucntions in MAGs.} Results of running the set of LR classsifiers trained on NCBI genomes on MAGs assembled from three environments: laboratory anaerobic digesters, the ocean and `candidate phyla radiation' (CPR) organisms from an aquifer system.\relax }{figure.caption.26}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces {\bf  Key genes and predicted functions for MAGs predicted to be methanogens.} Gene copy numbers for the \emph  {mcrA} methanogenesis gene and the energy-converting hyrdogenase A, along with functional predictions, for AD MAGs predicted to be mathanogenic by our algorithm.\relax }}{14}{table.caption.28}}
\newlabel{tab3}{{3}{14}{{\bf Key genes and predicted functions for MAGs predicted to be methanogens.} Gene copy numbers for the \emph {mcrA} methanogenesis gene and the energy-converting hyrdogenase A, along with functional predictions, for AD MAGs predicted to be mathanogenic by our algorithm.\relax }{table.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces {\bf  Overall comparison of AD, Tara and CPR MAGs.} Proportions of MAGS from the environments having a function, for some of the most common functions.\relax }}{15}{figure.caption.27}}
\newlabel{mag-compare}{{9}{15}{{\bf Overall comparison of AD, Tara and CPR MAGs.} Proportions of MAGS from the environments having a function, for some of the most common functions.\relax }{figure.caption.27}{}}
\bibstyle{plos2015}
\bibdata{trait_prediction,others}
\bibcite{Huttenhower2012}{1}
\bibcite{Gilbert2014}{2}
\bibcite{Zhang2015}{3}
\bibcite{Alneberg2014}{4}
\bibcite{Eren2015}{5}
\bibcite{Anantharaman2016}{6}
\bibcite{Kanehisa2017}{7}
\newlabel{S1_Fig}{{}{16}{S1 Fig}{section*.31}{}}
\bibcite{Finn2016}{8}
\bibcite{Tatusov1997}{9}
\bibcite{Louca2016}{10}
\bibcite{Louca2017}{11}
\bibcite{Louca2016a}{12}
\bibcite{Whitman}{13}
\bibcite{Leslie2002}{14}
\bibcite{Cai2003}{15}
\bibcite{Someya2010}{16}
\bibcite{Asgari2015}{17}
\bibcite{Weimann2016}{18}
\bibcite{Berger2005}{19}
\bibcite{Hyatt2010}{20}
\bibcite{Buchfink2014}{21}
\bibcite{Hastie2009a}{22}
\bibcite{Freedman2009}{23}
\bibcite{Lee2006}{24}
\bibcite{Lello2017}{25}
\bibcite{Hastie2009}{26}
\bibcite{Fawcett2006}{27}
\bibcite{Flach2011}{28}
\bibcite{Martiny2015}{29}
\newlabel{LastPage}{{}{18}{}{page.18}{}}
\xdef\lastpage@lastpage{18}
\xdef\lastpage@lastpageHy{18}
